{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzuABUaay5o0boYWuljUsi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/SNAP-CL/blob/main/03_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A Gentle Introduction to the Natural Language Toolkit**\n",
        "\n",
        "There are many different NLP/CL packages and libraries to choose from. We are going to work with one called NLTK - Natural Language Tool Kit. NLTK provides built-in functions for performing common NLP tasks, such as tokenising a text (splitting it into words), counting the frequency of words, part-of-speech tagging (e.g., assign words to nouns, verbs, etc), performing sentiment analysis, and so much more. \n",
        "\n",
        "We are going to touch just the surface of NLTK as a means to show you how to get going with some basic text analytics.\n",
        "\n",
        "> *You can learn Python and computational linguistics at the same time using the free NLTK book at https://www.nltk.org/book/*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SibtniHHnJwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading NLTK**\n",
        "\n",
        "We need to tell Python to load NLTK. To do so, we type `import nltk` in a code cell and run it, see below:\n"
      ],
      "metadata": {
        "id": "lpp2MVyia2LR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfnLXiVnnDaF"
      },
      "outputs": [],
      "source": [
        "# load the NLTK resource into the notebook\n",
        "import nltk "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to download some extra resources in order to use the NLTK functions in this notebook. Run the code cell below to download those resources. Because these notebooks are hosted on virtual servers, you would need to repeat this step each time you load this notebook. Fortunately, it does not take very long. Different functions will require different resources, and Colab will tell you if a resource is missing when you try to use NLTK functions. "
      ],
      "metadata": {
        "id": "75sljopiINrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download resources necessary for tokenizing and part of speech tagging.\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger'])"
      ],
      "metadata": {
        "id": "e4pIcXE5IcJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizing a Text**\n",
        "\n",
        "We can now use NLTK to split a string into separate words or *tokens*. We will do so using the `nltk.word_tokenize()` function. This function expects a string as the input, which you place inside the `()` at the end of the function, like so:"
      ],
      "metadata": {
        "id": "GwwZu-5nJD31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(\"These pretzels are making me thirsty!\")"
      ],
      "metadata": {
        "id": "bb9rgNMEID0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the output shows each word from the sentence separated by commas, and also that the punctuation mark \"!\" is treated as a separate word. The output is in the form of a Python `list`, another data structure which can be used to hold strings as well as other value types. \n",
        "\n",
        "Do you remember how you set a string to a variable? You can do the same thing with the results from functions, such as `nltk.word_tokenize()`. Consider below:"
      ],
      "metadata": {
        "id": "5Y-MOuW-J2p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save a string to a variable\n",
        "pretzels_raw = 'These pretzels are making me thirsty!'\n",
        "\n",
        "# save the tokenized version of the string held in pretzels_raw to a different variable\n",
        "pretzels_tokenized = nltk.word_tokenize(pretzels_raw)\n",
        "\n",
        "# inspect contents of tokenized version\n",
        "pretzels_tokenized"
      ],
      "metadata": {
        "id": "9YIpWgI9KNRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can thus query the length of our document, in words, using the `len()` function."
      ],
      "metadata": {
        "id": "euXhqXfVLNiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many words in our example? \n",
        "len(pretzels_tokenized)"
      ],
      "metadata": {
        "id": "ODk1BRZ_LQ56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**\n",
        "\n",
        "Of course, the punctuation is being counted as a \"word\", which we may not think is appropriate. \n",
        "\n",
        "This raises an important question regarding the computational analysis of text - how should texts be prepared before an analysis? Removing all of the punctuation from a text is a form of *pre-processing* and is commonly done in almost all natural language processing tasks. Other stages of pre-processing can include converting all words to lower case or removing so-called \"*stopwords*\", which are highly frequent *function* words such as *the*, *a*, *and*, and so on. Many existing NLP libraries / frameworks have option to conduct pre-processing automatically. \n",
        "\n",
        "Below, I have written a function which performs two stages of pre-processing: lowercasing and removing punctuation. Running the code cell will load the function into the notebook's memory so that you can use that same function in other code cells. "
      ],
      "metadata": {
        "id": "u6HaNQzdLVh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation markers we do not want\n",
        "punctuation = '!.,\\'\";:-'\n",
        "\n",
        "# define a function to pre-process text\n",
        "def preprocess(text):\n",
        "  # lower case the text and save results to a variable\n",
        "  lower_case = text.lower()\n",
        "  # remove punctuation from lower_case and save to a variable\n",
        "  # don't worry too much if you don't understand the code in this line. \n",
        "  lower_case_no_punctuation = ''.join([character for character in lower_case if character not in punctuation])\n",
        "  # return the new text to the user\n",
        "  return lower_case_no_punctuation"
      ],
      "metadata": {
        "id": "egx3mbaPNeMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next code cell, I use the `preprocess` function on a string which contains uppercase letters and one punctuation mark \"!\". The results show how all the letters are now lowercase, and the puncutation has been removed. "
      ],
      "metadata": {
        "id": "2bl79V2vjdAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test our function on a string\n",
        "preprocess('HELLO! wOrld.')"
      ],
      "metadata": {
        "id": "55SGiZ4WOUwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on, try out the preprocess function on some strings of your choice. You might want to try saving your string to a variable and then using the preprocess function, like this: \n",
        "\n",
        "> `my_variable = 'some string'`   \n",
        "> `preprocess(my_variable)`\n",
        "\n",
        "You might also want to try saving the results of preprocess to another variable, like this:\n",
        "\n",
        "> `new_variable = preprocess(my_variable)`"
      ],
      "metadata": {
        "id": "98cZd39qOtQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play with the preprocess() function here\n",
        "preprocess()"
      ],
      "metadata": {
        "id": "OkTo_oJsOyEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the preprocess function to process a text before sending it to be tokenized, such as seen below."
      ],
      "metadata": {
        "id": "XLwjpv1cSsrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save a string to a variable\n",
        "mood_ring = \"I can't feel a thing. I keep looking at my mood ring.\"\n",
        "\n",
        "# pre-process the string using the preprocess function, and save results to a variable\n",
        "mood_ring_preprocessed = preprocess(mood_ring)\n",
        "\n",
        "# tokenize the preprocessed text\n",
        "mood_ring_tokenized = nltk.word_tokenize(mood_ring_preprocessed)"
      ],
      "metadata": {
        "id": "NpBK64cSSxzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell shows you a comparison between the original string and the processed version. This provides a glimpse of the \"NLP pipeline\" we are building. "
      ],
      "metadata": {
        "id": "i3Km7o0hkCnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the original input and the eventual output\n",
        "print(f'Input\\n{mood_ring}\\n\\nOutput\\n{mood_ring_tokenized}')"
      ],
      "metadata": {
        "id": "ggjgiOpKTuWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Types and Tokens**\n",
        "\n",
        "Now that we can preprocess and tokenize a text, we can start querying properties of the texts. In this section we will consider how to count the frequency of different words in a text, as well as the overall lexical diversity of a text. Let's define some terms first:\n",
        "\n",
        "- A ***type*** is a unique word.\n",
        "- A ***token*** is an individual occurence of a type.\n",
        "\n",
        "\n",
        "For example - you might have three dogs: two Labradoodles and a Samoyed. If we sorted our dogs into types and tokens, we would have three tokens (three dogs), but only two types: Labradoodle or Samoyed.\n",
        "\n",
        "When we used `nltk.word_tokenize()`, we split our string into a series of tokens. \n",
        "\n",
        "We saw that we can also query the number of tokens by measuring the length of the tokenized list using `len()`\n",
        "\n",
        "For example, the number of tokens in our preprocessed example from above is 12, which we can confirm by manually counting the tokens.\n"
      ],
      "metadata": {
        "id": "m-kCQv4vK_m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mood_ring_tokenized"
      ],
      "metadata": {
        "id": "o9rJGTNDmZMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mood_ring_tokenized)"
      ],
      "metadata": {
        "id": "GNYWoIzxlTTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we figure out the number of types in that same example? We **could** manually count the number of types, which is 11 (because the token \"i\" occurs twice).\n",
        "\n",
        "We can also use a built-in Python function, `set()`, which returns a data container that only allows one of any value to exist in the container. In other words, it returns an object where repeated values are not allowed. This means we can simply use `set()` to ask for the unique values in our example. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YSGEMOuwlU-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the unique values among our tokens? \n",
        "set(mood_ring_tokenized)"
      ],
      "metadata": {
        "id": "B4z-FBQVn3hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then wrap `set()` inside `len()` to query how many types there are in our text. We see the answer is 11, which is one fewer than the number of tokens. "
      ],
      "metadata": {
        "id": "7k_dMesloArh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the length of the set of our tokens?\n",
        "len(set(mood_ring_tokenized))"
      ],
      "metadata": {
        "id": "NrEi2AeWoHYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Measuring Lexical Diversity**\n",
        "\n",
        "We can now use this information to assess our text for a very basic measure of sophistication: lexical diversity. This is also known as a type-token ratio, and provides a measure of how many repeated words there are in a text. You can read more about it in [Chapter 1 of NLTK.](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "To calculate lexical diveristy, we can use the following formula:\n",
        "\n",
        "> `number of types / number of tokens`\n",
        "\n",
        "In the code cell below, I create a function which calculates this value."
      ],
      "metadata": {
        "id": "lDyLumQool5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to calculate lexical diversity\n",
        "def lexical_diversity(tokens):\n",
        "  # return the result of dividing the length \n",
        "  return len(set(tokens))/len(tokens)"
      ],
      "metadata": {
        "id": "t3qz0s__uhGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore what the lexical diversity of our example is:\n"
      ],
      "metadata": {
        "id": "LryGaQWevz7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_diversity(mood_ring_tokenized)"
      ],
      "metadata": {
        "id": "XSOQwY3gv3Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a result of .916, in other words 91.6% of our tokens are represented by a single type, indicating a very high lexical diversity.\n",
        "\n",
        "Of course, such measures are relatively meaningless on such a short amount of text - the true use of lexical diversity would be to compare much larger texts against one another. One might also want to consider further pre-processing. \n",
        "\n",
        "Nonetheless, try the lexical diversity function on some examples yourself to see how repeating words influence the overall score. \n",
        "\n",
        "> ***Important!*** You need to feed a list of tokens to `lexical_diversity()`, otherwise you will get the diversity based on **characters** in the string, not words!"
      ],
      "metadata": {
        "id": "_fQSLMtDv6-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# play with lexical diversity here\n",
        "lexical_diversity(nltk.word_tokenize('hello world hello'))"
      ],
      "metadata": {
        "id": "xim13vzXwhSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which one will give us the total number of all the tokens in a text, and which one will give us the total number of types in a text? We can test this out without needing to rely on the NLTK objects."
      ],
      "metadata": {
        "id": "avYrVtUbn7bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needs vader lexicon\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6pR-k3UlI-N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "59WlgmirLi0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQdMM5gkLmiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "One of the most basic pieces of information we can ask from a text is the distribution of different words in a text (e.g., how frequent is each word, how diverse is a text's vocabulary), as well as basic properties of those words (e.g., noun, verb). \n",
        "\n",
        "To do so, we first need to understand how to separate a text into words. Remember, since Python sees any string as a sequence of characters (including whitespace and punctuation), this sequence does **not** understand words in the way that we do. \n",
        "\n",
        "We must therefore think of ways to split strings into separate words.\n",
        "\n",
        "We can use built-in methods to do so, such as the `string.split()` function in Python. But instead we will now shift to using a library called NLTK - Natural Language Tool Kit. \n"
      ],
      "metadata": {
        "id": "dWJb0Jz8QkVK"
      }
    }
  ]
}