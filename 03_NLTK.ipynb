{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvAy4asMpV3YVh5L1lzXSN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/SNAP-CL/blob/main/03_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A Gentle Introduction to the Natural Language Toolkit**\n",
        "\n",
        "There are many different NLP/CL packages and libraries to choose from. We are going to work with one called NLTK - Natural Language Tool Kit. NLTK provides built-in functions for performing common NLP tasks, such as tokenising a text (splitting it into words), counting the frequency of words, part-of-speech tagging (e.g., assign words to nouns, verbs, etc), performing sentiment analysis, and so much more. \n",
        "\n",
        "We are going to touch just the surface of NLTK as a means to show you how to get going with some basic text analytics.\n",
        "\n",
        "> *You can learn Python and computational linguistics at the same time using the free NLTK book at https://www.nltk.org/book/*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SibtniHHnJwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading NLTK**\n",
        "\n",
        "We need to tell Python to load NLTK. To do so, we type `import nltk` in a code cell and run it, see below:\n"
      ],
      "metadata": {
        "id": "lpp2MVyia2LR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfnLXiVnnDaF"
      },
      "outputs": [],
      "source": [
        "# load the NLTK resource into the notebook\n",
        "import nltk "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to download some extra resources in order to use the NLTK functions in this notebook. Run the code cell below to download those resources. Because these notebooks are hosted on virtual servers, you would need to repeat this step each time you load this notebook. Fortunately, it does not take very long. Different functions will require different resources, and Colab will tell you if a resource is missing when you try to use NLTK functions. "
      ],
      "metadata": {
        "id": "75sljopiINrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download resources necessary for tokenizing and part of speech tagging.\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4pIcXE5IcJJ",
        "outputId": "e72d9e79-0153-4952-b94d-1695b3bf30bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizing a Text**\n",
        "\n",
        "We can now use NLTK to split a string into separate words or *tokens*. We will do so using the `nltk.word_tokenize()` function. This function expects a string as the input, which you place inside the `()` at the end of the function, like so:"
      ],
      "metadata": {
        "id": "GwwZu-5nJD31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(\"These pretzels are making me thirsty!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9rgNMEID0b",
        "outputId": "d1fb0aa9-f7a2-48ce-ce8d-f723e56fed3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['These', 'pretzels', 'are', 'making', 'me', 'thirsty', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the output shows each word from the sentence separated by commas, and also that the punctuation mark \"!\" is treated as a separate word. The output is in the form of a Python `list`, another data structure which can be used to hold strings as well as other value types. \n",
        "\n",
        "Do you remember how you set a string to a variable? You can do the same thing with the results from functions, such as `nltk.word_tokenize()`. Consider below:"
      ],
      "metadata": {
        "id": "5Y-MOuW-J2p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save a string to a variable\n",
        "pretzels_raw = 'These pretzels are making me thirsty!'\n",
        "\n",
        "# save the tokenized version of the string held in pretzels_raw to a different variable\n",
        "pretzels_tokenized = nltk.word_tokenize(pretzels_raw)\n",
        "\n",
        "# inspect contents of tokenized version\n",
        "pretzels_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YIpWgI9KNRZ",
        "outputId": "ab6c7f27-d16b-4abd-e195-1593cdb84367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['These', 'pretzels', 'are', 'making', 'me', 'thirsty', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can thus query the length of our document, in words, using the `len()` function."
      ],
      "metadata": {
        "id": "euXhqXfVLNiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many words in our example? \n",
        "len(pretzels_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODk1BRZ_LQ56",
        "outputId": "3bbd751f-9c4f-4090-bbc0-97950c5a212a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Turn**\n",
        "\n",
        "Try tokenising some text and measuring the length using `len()`. You should explore feeding a raw string to the function as well as saving a string to a variable first. "
      ],
      "metadata": {
        "id": "XiLgzXYqKRh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize some text!"
      ],
      "metadata": {
        "id": "rDgc5uq4Kad_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**\n",
        "\n",
        "Of course, the punctuation is being counted as a \"word\", which we may not think is appropriate. \n",
        "\n",
        "This raises an important question regarding the computational analysis of text - how should texts be prepared before an analysis? Removing all of the punctuation from a text is a form of *pre-processing* and is commonly done in almost all natural language processing tasks. Other stages of pre-processing can include converting all words to lower case or removing so-called \"*stopwords*\", which are highly frequent *function* words such as *the*, *a*, *and*, and so on. Many existing NLP libraries / frameworks have option to conduct pre-processing automatically. \n",
        "\n",
        "Below, I have written a function which performs two stages of pre-processing: lowercasing and removing punctuation. Running the code cell will load the function into the notebook's memory so that you can use that same function in other code cells. "
      ],
      "metadata": {
        "id": "u6HaNQzdLVh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation markers we do not want\n",
        "punctuation = '!.,\\'\";:-'\n",
        "\n",
        "# define a function to pre-process text\n",
        "def preprocess(text):\n",
        "  # lower case the text and save results to a variable\n",
        "  lower_case = text.lower()\n",
        "  # remove punctuation from lower_case and save to a variable\n",
        "  # don't worry too much if you don't understand the code in this line. \n",
        "  lower_case_no_punctuation = ''.join([character for character in lower_case if character not in punctuation])\n",
        "  # return the new text to the user\n",
        "  return lower_case_no_punctuation"
      ],
      "metadata": {
        "id": "egx3mbaPNeMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next code cell, I use the `preprocess` function on a string which contains uppercase letters and one punctuation mark \"!\". The results show how all the letters are now lowercase, and the puncutation has been removed. "
      ],
      "metadata": {
        "id": "2bl79V2vjdAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test our function on a string\n",
        "preprocess('HELLO! wOrld.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "55SGiZ4WOUwI",
        "outputId": "83d8f699-f78c-4b9e-a031-47c6906c69ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on, try out the preprocess function on some strings of your choice. You might want to try saving your string to a variable and then using the preprocess function, like this: \n",
        "\n",
        "> `my_variable = 'some string'`   \n",
        "> `preprocess(my_variable)`\n",
        "\n",
        "You might also want to try saving the results of preprocess to another variable, like this:\n",
        "\n",
        "> `new_variable = preprocess(my_variable)`"
      ],
      "metadata": {
        "id": "98cZd39qOtQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play with the preprocess() function here\n",
        "#preprocess()"
      ],
      "metadata": {
        "id": "OkTo_oJsOyEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the preprocess function to process a text before sending it to be tokenized, such as seen below."
      ],
      "metadata": {
        "id": "XLwjpv1cSsrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save a string to a variable\n",
        "mood_ring = \"I can't feel a thing. I keep looking at my mood ring.\"\n",
        "\n",
        "# pre-process the string using the preprocess function, and save results to a variable\n",
        "mood_ring_preprocessed = preprocess(mood_ring)\n",
        "\n",
        "# tokenize the preprocessed text\n",
        "mood_ring_tokenized = nltk.word_tokenize(mood_ring_preprocessed)"
      ],
      "metadata": {
        "id": "NpBK64cSSxzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell shows you a comparison between the original string and the processed version. This provides a glimpse of the \"NLP pipeline\" we are building. "
      ],
      "metadata": {
        "id": "i3Km7o0hkCnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the original input and the eventual output\n",
        "print(f'Input\\n{mood_ring}\\n\\nOutput\\n{mood_ring_tokenized}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggjgiOpKTuWg",
        "outputId": "3a9ee296-19ef-44a1-be2e-c00694f1183a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input\n",
            "I can't feel a thing. I keep looking at my mood ring.\n",
            "\n",
            "Output\n",
            "['i', 'cant', 'feel', 'a', 'thing', 'i', 'keep', 'looking', 'at', 'my', 'mood', 'ring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Turn**\n",
        "\n",
        "Play with the preprocess function to compare the before and after of different text."
      ],
      "metadata": {
        "id": "8HaZoH74MwMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try the preprocess function here\n"
      ],
      "metadata": {
        "id": "JFr11mxQM2T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Types and Tokens**\n",
        "\n",
        "Now that we can preprocess and tokenize a text, we can start querying properties of the texts. In this section we will consider how to count the frequency of different words in a text, as well as the overall lexical diversity of a text. Let's define some terms first:\n",
        "\n",
        "- A ***type*** is a unique word.\n",
        "- A ***token*** is an individual occurence of a type.\n",
        "\n",
        "\n",
        "For example - you might have three dogs: two Labradoodles and a Samoyed. If we sorted our dogs into types and tokens, we would have three tokens (three dogs), but only two types: Labradoodle or Samoyed.\n",
        "\n",
        "When we used `nltk.word_tokenize()`, we split our string into a series of tokens. \n",
        "\n",
        "We saw that we can also query the number of tokens by measuring the length of the tokenized list using `len()`\n",
        "\n",
        "For example, the number of tokens in our preprocessed example from above is 12, which we can confirm by manually counting the tokens.\n"
      ],
      "metadata": {
        "id": "m-kCQv4vK_m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mood_ring_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9rJGTNDmZMk",
        "outputId": "f4eb9f8d-0e13-4cc5-dbc5-3736d8a26a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'cant',\n",
              " 'feel',\n",
              " 'a',\n",
              " 'thing',\n",
              " 'i',\n",
              " 'keep',\n",
              " 'looking',\n",
              " 'at',\n",
              " 'my',\n",
              " 'mood',\n",
              " 'ring']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(mood_ring_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNYWoIzxlTTp",
        "outputId": "06968428-5d53-4b90-df00-9c69a79288a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we figure out the number of types in that same example? We **could** manually count the number of types, which is 11 (because the token \"i\" occurs twice).\n",
        "\n",
        "We can also use a built-in Python function, `set()`, which returns a data container that only allows one of any value to exist in the container. In other words, it returns an object where repeated values are not allowed. This means we can simply use `set()` to ask for the unique values in our example. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YSGEMOuwlU-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the unique values among our tokens? \n",
        "set(mood_ring_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4z-FBQVn3hZ",
        "outputId": "348854fb-8b50-471e-abf5-3584b2024eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'at',\n",
              " 'cant',\n",
              " 'feel',\n",
              " 'i',\n",
              " 'keep',\n",
              " 'looking',\n",
              " 'mood',\n",
              " 'my',\n",
              " 'ring',\n",
              " 'thing'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then wrap `set()` inside `len()` to query how many types there are in our text. We see the answer is 11, which is one fewer than the number of tokens. "
      ],
      "metadata": {
        "id": "7k_dMesloArh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the length of the set of our tokens?\n",
        "len(set(mood_ring_tokenized))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrEi2AeWoHYN",
        "outputId": "878a52d7-3d7b-4b83-f0ba-00a29116a4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Turn**\n",
        "\n",
        "Compare the results of `len()` and `set()` on different strings of your choosing. "
      ],
      "metadata": {
        "id": "ApVl4UglM5i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare len() and set() here. \n"
      ],
      "metadata": {
        "id": "Bdxsda4FM-i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Measuring Lexical Diversity**\n",
        "\n",
        "We can now use this information to assess our text for a very basic measure of sophistication: lexical diversity. This is also known as a type-token ratio, and provides a measure of how many repeated words there are in a text. You can read more about it in [Chapter 1 of NLTK.](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "To calculate lexical diveristy, we can use the following formula:\n",
        "\n",
        "> `number of types / number of tokens`\n",
        "\n",
        "In the code cell below, I create a function which calculates this value."
      ],
      "metadata": {
        "id": "lDyLumQool5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to calculate lexical diversity\n",
        "def lexical_diversity(tokens):\n",
        "  # return the result of dividing the length \n",
        "  return len(set(tokens))/len(tokens)"
      ],
      "metadata": {
        "id": "t3qz0s__uhGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore what the lexical diversity of our example is:\n"
      ],
      "metadata": {
        "id": "LryGaQWevz7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_diversity(mood_ring_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSOQwY3gv3Dd",
        "outputId": "4b7d5777-3a89-44ee-89a8-1619a6c07337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9166666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a result of .916, in other words 91.6% of our tokens are represented by a single type, indicating a very high lexical diversity.\n",
        "\n",
        "Of course, such measures are relatively meaningless on such a short amount of text - the true use of lexical diversity would be to compare much larger texts against one another. One might also want to consider further pre-processing. \n",
        "\n",
        "Nonetheless, try the lexical diversity function on some examples yourself to see how repeating words influence the overall score. \n",
        "\n",
        "> ***Important!*** You need to feed a list of tokens to `lexical_diversity()`, otherwise you will get the diversity based on **characters** in the string, not words!"
      ],
      "metadata": {
        "id": "_fQSLMtDv6-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical diversity of 50%\n",
        "lexical_diversity(nltk.word_tokenize('hello world hello world'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xim13vzXwhSu",
        "outputId": "851353e9-be7c-4d55-aa8d-2b8cc70ff153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical diversity of 100%\n",
        "lexical_diversity(nltk.word_tokenize('hello world'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8_JWEvf0NR0",
        "outputId": "59b30c39-46ab-444f-ff31-62ccd9c532a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Turn**\n",
        "\n",
        "Try out the lexical diversity function on some text. The function expects raw string as input. "
      ],
      "metadata": {
        "id": "T9PgkjmnNBUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play with lexical diversity on your own examples\n",
        "\n"
      ],
      "metadata": {
        "id": "YxHKIMiS0kxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Frequency**\n",
        "\n",
        "We can also query the frequency of tokens in a text using an NLTK function. We will again feed a list of tokens to this function. The syntax for this function is:\n",
        "\n",
        "> `nltk.FreqDist(tokens)`\n",
        "\n",
        "Consider the following example:"
      ],
      "metadata": {
        "id": "ETv_02kq1BCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# will this become stuck in your head?\n",
        "turtles = \"\"\"teenage mutant ninja turtles, \n",
        "            teenage mutant ninja turtles, \n",
        "            teenage mutant ninja turtles, \n",
        "            heroes in a halfshell, turtle power!\"\"\"\n",
        "\n",
        "\n",
        "# save the frequency distribution to a variable\n",
        "turtle_fdist = nltk.FreqDist(nltk.word_tokenize(turtles))\n",
        "\n",
        "# inspect the results\n",
        "turtle_fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk_kCrH82G6N",
        "outputId": "6874b07b-c108-4399-b785-91758e604caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 4, 'teenage': 3, 'mutant': 3, 'ninja': 3, 'turtles': 3, 'heroes': 1, 'in': 1, 'a': 1, 'halfshell': 1, 'turtle': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting frequency distribution is another Python data object called a `dictionary` which stores key:value pairs. In this case, our keys are the words, and the values are the frequencies.\n",
        "\n",
        "We can query a dictionary for specific key:value pairs using the following syntax:\n",
        "\n",
        "> `dictionary['key']`\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "zuJX3Rz13bj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtles?\"\n",
        "turtle_fdist['turtles']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv8qr37a3uw9",
        "outputId": "5bd40d00-1371-4256-d910-960e2a472ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtle?\"\n",
        "\n",
        "turtle_fdist['turtle']"
      ],
      "metadata": {
        "id": "pCqS9l6l39yz",
        "outputId": "ea827ea3-5dd8-4b90-bfa6-0c978751b974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also ask for the most frequent N terms from a frequency distribution using the `.most_common()` method. We can specific the number of top results we want by putting a number in the brackets `()` used by `.most_common()`. Below I ask for the number one most common word in our example:"
      ],
      "metadata": {
        "id": "kMSZrRTE4Csl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turtle_fdist.most_common(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swc9GHLj4GUB",
        "outputId": "414ff601-74d5-494a-b3d0-9224ac00b327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it turns out, the most common \"word\" was a comma. Yet another example of why pre-processing is an important step in text analytics and NLP. \n",
        "\n",
        "\n",
        "**Your turn**\n",
        "\n",
        "Take this opportunity to make your own frequency distributions using `nltk.FreqDist()`. Remember to supply the function with a list of tokens - if you're curious you can see what happens if you supply a raw string!"
      ],
      "metadata": {
        "id": "TgOteauK4UU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play with FreqDist here. \n",
        "\n"
      ],
      "metadata": {
        "id": "hfLFr44M4BTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parts of Speech**\n",
        "\n",
        "Words are classified into different word categories, such as nouns, verbs, adjectives, pronouns, etc. These annotations are called parts of speech (POS) and are another source of information used in NLP applications. \n",
        "\n",
        "\n",
        "You can think of the part of speech tags as additional information about a word - which can then also be counted and compared, but is also critical information for building and understanding grammars of languages. Tagging is a fundamental part of the NLP pipeline and usually the step which occurs after tokenization.\n",
        "\n",
        "Today, most tagging (and tokenization) is done by using large language models which represent words as numerical features in a vector space. We aren't going to get into that - we'll just use the built in NLTK part of speech tagging function. \n",
        "\n",
        "The function expects tokens:\n",
        "\n",
        "> `nltk.pos_tag(tokens)`\n",
        "\n",
        "The results will be a list of `(word,tag)` pairs (which incidently introduces you to another Python data structure, the tuple.)\n"
      ],
      "metadata": {
        "id": "G7XKg53_5d6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of Speech (POS) tags for our example\n",
        "nltk.pos_tag(mood_ring_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytvl9K_nGkzl",
        "outputId": "2813a91d-981f-4754-bb58-7b20423a2aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'NN'),\n",
              " ('cant', 'VBP'),\n",
              " ('feel', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('thing', 'NN'),\n",
              " ('i', 'NN'),\n",
              " ('keep', 'VBP'),\n",
              " ('looking', 'VBG'),\n",
              " ('at', 'IN'),\n",
              " ('my', 'PRP$'),\n",
              " ('mood', 'NN'),\n",
              " ('ring', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see in the output the POS tags are represented as strings such as \"NN\" and \"VBP\". These all stand for different parts of speech. You can view the built-in help for what tag means by running the code in the next cell or by [going here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
      ],
      "metadata": {
        "id": "ou2UcDYCHVIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# full list of tags, with definitions and examples\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "rQHysxETHmC1",
        "outputId": "f952e826-6bdc-45de-c8af-81c232d93933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of speech tags help make sense of words in the context of other words. One way tags are helpful is to distinguish different meanings/uses of words which can be used in different parts of speech. For example:"
      ],
      "metadata": {
        "id": "KLzeBkKRIHx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what pos tag does the word \"comb\" have in this example?\n",
        "nltk.pos_tag(nltk.word_tokenize('Quick, comb the desert for droids!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-zzcRuIIG_s",
        "outputId": "e3b17e50-05b2-4c02-d440-9b87649be6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Quick', 'NNP'),\n",
              " (',', ','),\n",
              " ('comb', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('desert', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('droids', 'NNS'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and what pos tag does the word \"comb\" have in this example?\n",
        "nltk.pos_tag(nltk.word_tokenize('Where is my comb?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sZL8rR0IcOZ",
        "outputId": "b32402f5-d338-4a87-894c-ccfedff45b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Where', 'WRB'), ('is', 'VBZ'), ('my', 'PRP$'), ('comb', 'NN'), ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, adding POS tag information provides more information about a text, which becomes useful for more advanced NLP applications such as information extraction, text prediction, and so on. Because the tags are stores as strings, you can use knowledge of Python to search or filter through the list in order to find specific words associated with specific tags. \n",
        "\n",
        "We could even feed this to a frequency distribution and see how frequently certain words appear with certain POS tags. "
      ],
      "metadata": {
        "id": "TPOnAeZTIslo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.FreqDist(nltk.pos_tag(nltk.word_tokenize('Where is my comb? Please comb the desert for droids!')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbX7juAMJ995",
        "outputId": "0cb65a73-506f-4b8b-d722-954b0fd1a7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('Where', 'WRB'): 1, ('is', 'VBZ'): 1, ('my', 'PRP$'): 1, ('comb', 'NN'): 1, ('?', '.'): 1, ('Please', 'NNP'): 1, ('comb', 'VBZ'): 1, ('the', 'DT'): 1, ('desert', 'NN'): 1, ('for', 'IN'): 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Turn**\n",
        "\n",
        "Use the part of speech function to tag the part of speech of some text. Remember that you need to provide `nltk.pos_tag()` a list of tokens."
      ],
      "metadata": {
        "id": "MpxHTtgKNQzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at different POS tags here.\n"
      ],
      "metadata": {
        "id": "IWLgA1tJNZbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A full NLP Pipeline**\n",
        "\n",
        "Let's combine everythign we've done into a full NLP pipeline which reads in raw text (as a string) and then provides information about that text. I will create a function which applies pre-processing and then outputs various information about a text. To do so, I will create a new function which contains the `preprocess` and `lexical_diveristy` functions we used above, as well as output the top 5 frequent word:pos_tag combinations."
      ],
      "metadata": {
        "id": "38z0oodmLbb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(string_input):\n",
        "  # first lowercase the string and clear punctuation using our preprocess function (defined above)\n",
        "  preprocess_string = preprocess(string_input)\n",
        "\n",
        "  # now use NLTK to tokenize the preprocessed text\n",
        "  tokenized_string = nltk.word_tokenize(preprocess_string)\n",
        "\n",
        "  # calculate the diversity function (defined above)\n",
        "  ld = lexical_diversity(tokenized_string)\n",
        "\n",
        "  # pos tag the tokens\n",
        "  pos_tagged_string = nltk.pos_tag(tokenized_string)\n",
        "\n",
        "  # calculate frequency of words and tags\n",
        "  fdist = nltk.FreqDist(pos_tagged_string)\n",
        "\n",
        "  # output some information about the text\n",
        "  print(f\"\"\"\n",
        "  Length:\\t{len(tokenized_string)}\\n\n",
        "  Lexical Diversity:\\t{ld}\\n\n",
        "  Top 5 Frequent Words:\\t{fdist.most_common(5)}\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "VaTClXqLLtt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then apply the function to a longer text below, and quickly get statistics such as total length, lexical diversity, and the top five most frequent words. "
      ],
      "metadata": {
        "id": "eoUgPdjMOc2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "george = \"\"\"The sea was angry that day, my friends - like an old man trying to send back soup in a deli. \n",
        "I got about fifty feet out and suddenly, the great beast appeared before me. \n",
        "I tell you, he was ten stories high if he was a foot. \n",
        "As if sensing my presence, he let out a great bellow.\"\"\"\n",
        "\n",
        "# get info about this text.\n",
        "pipeline(george)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijrsN153NVye",
        "outputId": "31a904c7-d052-4f2c-9265-b28b5c2987e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Length:\t58\n",
            "\n",
            "  Lexical Diversity:\t0.7931034482758621\n",
            "\n",
            "  Top 5 Frequent Words:\t[(('was', 'VBD'), 3), (('a', 'DT'), 3), (('he', 'PRP'), 3), (('the', 'DT'), 2), (('my', 'PRP$'), 2)]\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Turn**\n",
        "\n",
        "You can modify the function above (or remake your own) to provide information most relevant to your research interests or questions! Once you learn more Python, you can add options to either keep or remove punctuation, calculate frequency from only the words, or anything else. What sorts of information might you want to ask from your texts? "
      ],
      "metadata": {
        "id": "juXOOLb4Owd3"
      }
    }
  ]
}