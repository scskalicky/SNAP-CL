{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTKKrnFVdlIggCl26PEyZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/SNAP-CL/blob/main/03_Frequency_Tokenisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using NLTK to Tokenize and Tag Text**\n",
        "\n",
        "One of the most basic pieces of information we can ask from a text is the distribution of differnt words in a text (e.g., how frequent is each word, how diverse is a text's vocabulary), as well as basic properties of those words (e.g., noun, verb). \n",
        "\n",
        "To do so, we first need to understand how to separate a text into words. Remember, since Python sees any string as a sequence of characters (including whitespace and punctuation), this sequence does **not** understand words in the way that we do. \n",
        "\n",
        "We must therefore think of ways to split strings into separate words.\n",
        "\n",
        "We can use built-in methods to do so, such as the `string.split()` function in Python. But instead we will now shift to using a library called NLTK - Natural Language Tool Kit. \n",
        "\n",
        "> *You can learn Python and computational linguistics at the same time using their free book at https://www.nltk.org/book/*\n",
        "\n",
        "\n",
        "## **NLTK**\n",
        "\n",
        "We need to tell Python to load NLTK. To do so, we type `import nltk` in a code cell, see below:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SibtniHHnJwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfnLXiVnnDaF"
      },
      "outputs": [],
      "source": [
        "# load the NLTK resource into the notebook\n",
        "import nltk "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to download some extra resources in order to use the NLTK functions. Run the code cell below to download those resources. Because these notebooks are hosted on virtual servers, you would need to repeat this step each time."
      ],
      "metadata": {
        "id": "75sljopiINrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download resources necessary for tokenizing and part of speech tagging.\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger'])"
      ],
      "metadata": {
        "id": "e4pIcXE5IcJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizing a text**\n",
        "\n",
        "We can now split a string into separate words or *tokens*. We will do so using the `nltk.word_tokenize()` function. Put the string inside the `()` at the end of the function, like so:"
      ],
      "metadata": {
        "id": "GwwZu-5nJD31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(\"These pretzels are making me thirsty!\")"
      ],
      "metadata": {
        "id": "bb9rgNMEID0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the output shows each word from the sentence separated by commas, and also that the punctuation mark \"!\" is treated as a separate word. The output is in the form of a Python `list`, another data structure which can be used to hold strings as well as other data types. \n",
        "\n",
        "Do you remember how you set a string to a variable? You can do the same thing with the results from functions, such as `nltk.word_tokenize()`. Consider below."
      ],
      "metadata": {
        "id": "5Y-MOuW-J2p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save a string to a variable\n",
        "pretzels_raw = 'These pretzels are making me thirsty!'\n",
        "\n",
        "# save the tokenized version of the string to a different variable\n",
        "pretzels_tokenized = nltk.word_tokenize(pretzels_raw)\n",
        "\n",
        "# inspect contents of tokenized version\n",
        "pretzels_tokenized"
      ],
      "metadata": {
        "id": "9YIpWgI9KNRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can thus query the length of our document, in words, using the `len()` function."
      ],
      "metadata": {
        "id": "euXhqXfVLNiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many words in our example? \n",
        "len(pretzels_tokenized)"
      ],
      "metadata": {
        "id": "ODk1BRZ_LQ56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, the punctuation is being counted as a \"word\", which we may not think is appropriate. \n",
        "\n",
        "This raises an important question regarding the computational analysis of text - how should texts be prepared before an analysis. Removing all of the punctuation from a text is a form of *pre-processing* and is commonly done in almost all natural language processing tasks. Other stages of pre-processing can include converting all words to lower case or removing so-called \"*stopwods*\", which are highly frequent *function* words such as *the*, *a*, *and*, and so on. Many existing NLP libraries / frameworks have option to conduct pre-processing automatically. \n",
        "\n",
        "Below, I have written a function which performs two stages of pre-processing: lowercasing and removing punctuation."
      ],
      "metadata": {
        "id": "u6HaNQzdLVh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation markers we do not want\n",
        "punctuation = '!.,\\'\";:-'\n",
        "\n",
        "# define a function to pre-process text\n",
        "def preprocess(text):\n",
        "  # lower case the text and save results to a variable\n",
        "  lower_case = text.lower()\n",
        "  # remove punctuation from lower_case and save to a variable\n",
        "  # don't worry too much if you don't understand the code in this line. \n",
        "  lower_case_no_punctuation = ''.join([character for character in lower_case if character not in punctuation])\n",
        "  # return the new text to the user\n",
        "  return lower_case_no_punctuation"
      ],
      "metadata": {
        "id": "egx3mbaPNeMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test our function on a string\n",
        "preprocess('HELLO! wOrld.')"
      ],
      "metadata": {
        "id": "55SGiZ4WOUwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on, try out the preprocess function on some strings of your choice. You might want to try saving your string to a variable and then using the preprocess function, like this: \n",
        "\n",
        "> `preprocess(variable)`\n",
        "\n",
        "You might also want to try saving the results of preprocess to another variable, like this:\n",
        "\n",
        "> `new_variable = preprocess(variable)`"
      ],
      "metadata": {
        "id": "98cZd39qOtQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play with the preprocess() function here\n",
        "preprocess()"
      ],
      "metadata": {
        "id": "OkTo_oJsOyEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vocabulary - Tokens and Types**\n",
        "What is the difference between a token and a type?\n",
        "\n",
        "- A token is an individual occurence of a word.\n",
        "- A type is the actual word.\n",
        "\n",
        "For example - you might have three dogs: two Labradoodles and a Samoyed.\n",
        "\n",
        "You would have three tokens (three dogs) but only two types: Labradoodle or Samoyed.\n",
        "\n",
        "NLKT asks you to think about types and tokes by introducing you to the set and len functions. Do you remember these?\n",
        "\n",
        "Which one will give us the total number of all the tokens in a text, and which one will give us the total number of types in a text? We can test this out without needing to rely on the NLTK objects."
      ],
      "metadata": {
        "id": "m-kCQv4vK_m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needs vader lexicon\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6pR-k3UlI-N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "59WlgmirLi0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQdMM5gkLmiV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}