{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtNfyOQ5fqKz5Lc9HDNGw3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/SNAP-CL/blob/main/05_Loading_Texts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading in a longer text**\n",
        "It's now time to start thinking about how to get your own data into Python/NLTK so you can analyse it. \n",
        "\n",
        "\n",
        "Navigate to [this page](https://raw.githubusercontent.com/scskalicky/SNAP-CL/main/tmoom.txt). You should see some text. Copy and paste the text into a text editor and save it to your desktop (or somewhere else on your computer). Name the file `tmoom.txt`. Make sure the file is a `.txt` file. \n",
        "\n",
        "Now that we have a text file, the next step is to put the file somewhere that the Colab notebook can reach. To do so, click on the file folder icon on the left hand side of the window. The menu should expand and you will see a folder called \"sample_data\" which was loaded with this notebook. \n",
        "\n",
        "You will also see three icons above which, from left to right, let you upload material into the current session, refresh the list of files, mount your google drive, and show hidden files.\n",
        "\n",
        "The first option is rather easy and allows you to upload the `tmoom.txt` file direclty into the drive space. You will get a warning telling you that the file will be removed whever the notebook closes. \n",
        "\n",
        "The third option (the grey file folder with the google drive icon) allows you to \"mount\" your google drive. This provides temporary access between the notebook and your entire google drive, which then allows for you to read and write files from your drive which will not be removed once the notebook session ends. \n",
        "\n",
        "Whichever option you choose, we can then use the `open()` function to load in the file. You will need to provide the path to the file inside the brackets, and this needs to be typed as a string. If you choose the first option and upload the text into the notebook, you should be able to run the cell below and get the file into the workspace.\n",
        "\n",
        "Because we want to store the *contents* of the .txt file to a variable, we will append `.read()` to the end of `open()`.\n"
      ],
      "metadata": {
        "id": "NOJjlTgaLSYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ9jyAqjLNRV"
      },
      "outputs": [],
      "source": [
        "# Load in the contents of the .txt file and save it as a variable\n",
        "tmoom = open('tmoom.txt').read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the variable - it is saved as a single string\n",
        "tmoom"
      ],
      "metadata": {
        "id": "2aREKW-bYNEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now operate on this text as we have done with out previous examples. Since this is a new notebook, we'll need to reload NLTK and its resources first. "
      ],
      "metadata": {
        "id": "fqMNKIvDYm5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load NLTK library\n",
        "import nltk\n",
        "# download resources necessary for tokenizing and part of speech tagging.\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets'])"
      ],
      "metadata": {
        "id": "5jxP07t4Yw0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load in the same preprocess, lexical diversity, and pipeline functions as before:"
      ],
      "metadata": {
        "id": "UekpXed1Y5S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation markers we do not want\n",
        "punctuation = '!.,\\'\";:-'\n",
        "\n",
        "# define a function to pre-process text\n",
        "def preprocess(text):\n",
        "  # lower case the text and save results to a variable\n",
        "  lower_case = text.lower()\n",
        "  # remove punctuation from lower_case and save to a variable\n",
        "  # don't worry too much if you don't understand the code in this line. \n",
        "  lower_case_no_punctuation = ''.join([character for character in lower_case if character not in punctuation])\n",
        "  # return the new text to the user\n",
        "  return lower_case_no_punctuation\n",
        "\n",
        "\n",
        "\n",
        "# define a function to calculate lexical diversity\n",
        "def lexical_diversity(tokens):\n",
        "  # return the result of dividing the length \n",
        "  return len(set(tokens))/len(tokens)\n",
        "\n",
        "\n",
        "\n",
        "def pipeline(string_input):\n",
        "  # first lowercase the string and clear punctuation using our preprocess function (defined above)\n",
        "  preprocess_string = preprocess(string_input)\n",
        "\n",
        "  # now use NLTK to tokenize the preprocessed text\n",
        "  tokenized_string = nltk.word_tokenize(preprocess_string)\n",
        "\n",
        "  # calculate the diversity function (defined above)\n",
        "  ld = lexical_diversity(tokenized_string)\n",
        "\n",
        "  # pos tag the tokens\n",
        "  pos_tagged_string = nltk.pos_tag(tokenized_string)\n",
        "\n",
        "  # calculate frequency of words and tags\n",
        "  fdist = nltk.FreqDist(pos_tagged_string)\n",
        "\n",
        "  # output some information about the text\n",
        "  print(f\"\"\"\n",
        "  Length:\\t{len(tokenized_string)}\\n\n",
        "  Lexical Diversity:\\t{ld}\\n\n",
        "  Top 5 Frequent Words:\\t{fdist.most_common(5)}\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "egx3mbaPNeMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila, we can now operate on our text as we have been doing. "
      ],
      "metadata": {
        "id": "Ir3XzcdzZNEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(tmoom)"
      ],
      "metadata": {
        "id": "219jCxjcZCee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you choose to mount your Google Drive, you only need to change the file path to match wherever the file is on your Google Drive.\n",
        "\n",
        "The start of your file path will always be `content/drive/MyDrive/...`, where the `...` represents the root level of your Google Drive. \n",
        "\n",
        "So if you had `tmoom.txt` saved in a folder named `texts` in your Google Drive, the filepath would be:\n",
        "\n",
        "`open('/content/drive/MyDrive/texts/tmoom.txt').read()`"
      ],
      "metadata": {
        "id": "JbrhzJxAZ8JB"
      }
    }
  ]
}