{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQeAesDkoYjTDKcsGRIYTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/SNAP-CL/blob/main/05_Loading_Texts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading in a longer text**\n",
        "It's now time to start thinking about how to get your own data into Python/NLTK so you can analyse it. \n",
        "\n",
        "\n",
        "Navigate to [this page](https://raw.githubusercontent.com/scskalicky/SNAP-CL/main/tmoom.txt). You should see some text. Copy and paste the text into a text editor and save it to your desktop (or somewhere else on your computer). Name the file `tmoom.txt`. Make sure the file is a `.txt` file. \n",
        "\n",
        "Now that we have a text file, the next step is to put the file somewhere that the Colab notebook can reach. To do so, click on the file folder icon on the left hand side of the window. The menu should expand and you will see a folder called \"sample_data\" which was loaded with this notebook. \n",
        "\n",
        "You will also see three icons above which, from left to right, let you upload material into the current session, refresh the list of files, mount your google drive, and show hidden files.\n",
        "\n",
        "The first option is rather easy and allows you to upload the `tmoom.txt` file direclty into the drive space. You will get a warning telling you that the file will be removed whever the notebook closes. \n",
        "\n",
        "The third option (the grey file folder with the google drive icon) allows you to \"mount\" your google drive. This provides temporary access between the notebook and your entire google drive, which then allows for you to read and write files from your drive which will not be removed once the notebook session ends. \n",
        "\n",
        "Whichever option you choose, we can then use the `open()` function to load in the file. You will need to provide the path to the file inside the brackets, and this needs to be typed as a string. If you choose the first option and upload the text into the notebook, you should be able to run the cell below and get the file into the workspace.\n",
        "\n",
        "Because we want to store the *contents* of the .txt file to a variable, we will append `.read()` to the end of `open()`.\n"
      ],
      "metadata": {
        "id": "NOJjlTgaLSYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IZ9jyAqjLNRV"
      },
      "outputs": [],
      "source": [
        "# Load in the contents of the .txt file and save it as a variable\n",
        "tmoom = open('tmoom.txt').read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the variable - it is saved as a single string\n",
        "tmoom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "2aREKW-bYNEv",
        "outputId": "8fa7908c-2640-40e7-cdfd-ac3770c6fa43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"They\\'re made out of meat.\"\\n\"Meat?\"\\n\"Meat. They\\'re made out of meat.\"\\n\"Meat?\"\\n\"There\\'s no doubt about it. We picked several from different parts of the planet, took them aboard our recon vessels, probed them all the way through. They\\'re completely meat.\"\\n\"That\\'s impossible. What about the radio signals? The messages to the stars.\"\\n\"They use the radio waves to talk, but the signals don\\'t come from them. The signals come from machines.\"\\n\"So who made the machines? That\\'s who we want to contact.\"\\n\"They made the machines. That\\'s what I\\'m trying to tell you. Meat made the machines.\"\\n\"That\\'s ridiculous. How can meat make a machine? You\\'re asking me to believe in sentient meat.\"\\n\"I\\'m not asking you, I\\'m telling you. These creatures are the only sentient race in the sector and they\\'re made out of meat.\"\\n\"Maybe they\\'re like the Orfolei. You know, a carbon-based intelligence that goes through a meat stage.\"\\n\"Nope. They\\'re born meat and they die meat. We studied them for several of their life spans, which didn\\'t take too long. Do you have any idea the life span of meat?\"\\n\"Spare me. Okay, maybe they\\'re only part meat. You know, like the Weddilei. A meat head with an electron plasma brain inside.\"\\n\"Nope. We thought of that, since they do have meat heads like the Weddilei. But I told you, we probed them. They\\'re meat all the way through.\"\\n\"No brain?\"\\n\"Oh, there is a brain all right. It\\'s just that the brain is made out of meat!\"\\n\"So... what does the thinking?\"\\n\"You\\'re not understanding, are you? The brain does the thinking. The meat.\"\\n\"Thinking meat! You\\'re asking me to believe in thinking meat!\"\\n\"Yes, thinking meat! Conscious meat! Loving meat. Dreaming meat. The meat is the whole deal! Are you getting the picture?\"\\n\"Omigod. You\\'re serious then. They\\'re made out of meat.\"\\n\"Finally, Yes. They are indeed made out meat. And they\\'ve been trying to get in touch with us for almost a hundred of their years.\"\\n\"So what does the meat have in mind.\"\\n\"First it wants to talk to us. Then I imagine it wants to explore the universe, contact other sentients, swap ideas and information. The usual.\"\\n\"We\\'re supposed to talk to meat?\"\\n\"That\\'s the idea. That\\'s the message they\\'re sending out by radio. \\'Hello. Anyone out there? Anyone home?\\' That sort of thing.\"\\n\"They actually do talk, then. They use words, ideas, concepts?\"\\n\"Oh, yes. Except they do it with meat.\"\\n\"I thought you just told me they used radio.\"\\n\"They do, but what do you think is on the radio? Meat sounds. You know how when you slap or flap meat it makes a noise? They talk by flapping their meat at each other. They can even sing by squirting air through their meat.\"\\n\"Omigod. Singing meat. This is altogether too much. So what do you advise?\"\\n\"Officially or unofficially?\"\\n\"Both.\"\\n\"Officially, we are required to contact, welcome, and log in any and all sentient races or multibeings in the quadrant, without prejudice, fear, or favor. Unofficially, I advise that we erase the records and forget the whole thing.\"\\n\"I was hoping you would say that.\"\\n\"It seems harsh, but there is a limit. Do we really want to make contact with meat?\"\\n\"I agree one hundred percent. What\\'s there to say?\" `Hello, meat. How\\'s it going?\\' But will this work? How many planets are we dealing with here?\"\\n\"Just one. They can travel to other planets in special meat containers, but they can\\'t live on them. And being meat, they only travel through C space. Which limits them to the speed of light and makes the possibility of their ever making contact pretty slim. Infinitesimal, in fact.\"\\n\"So we just pretend there\\'s no one home in the universe.\"\\n\"That\\'s it.\"\\n\"Cruel. But you said it yourself, who wants to meet meat? And the ones who have been aboard our vessels, the ones you have probed? You\\'re sure they won\\'t remember?\"\\n\"They\\'ll be considered crackpots if they do. We went into their heads and smoothed out their meat so that we\\'re just a dream to them.\"\\n\"A dream to meat! How strangely appropriate, that we should be meat\\'s dream.\"\\n\"And we can marked this sector unoccupied.\"\\n\"Good. Agreed, officially and unofficially. Case closed. Any others? Anyone interesting on that side of the galaxy?\"\\n\"Yes, a rather shy but sweet hydrogen core cluster intelligence in a class nine star in G445 zone. Was in contact two galactic rotation ago, wants to be friendly again.\"\\n\"They always come around.\"\\n\"And why not? Imagine how unbearably, how unutterably cold the universe would be if one were all alone.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now operate on this text as we have done with out previous examples. Since this is a new notebook, we'll need to reload NLTK and its resources first. "
      ],
      "metadata": {
        "id": "fqMNKIvDYm5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load NLTK library\n",
        "import nltk\n",
        "# download resources necessary for tokenizing and part of speech tagging.\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jxP07t4Yw0N",
        "outputId": "c8bcc604-1e80-4159-892a-6b3043d91a45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load in the same preprocess, lexical diversity, and pipeline functions as before:"
      ],
      "metadata": {
        "id": "UekpXed1Y5S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation markers we do not want\n",
        "punctuation = '!.,\\'\";:-'\n",
        "\n",
        "# define a function to pre-process text\n",
        "def preprocess(text):\n",
        "  # lower case the text and save results to a variable\n",
        "  lower_case = text.lower()\n",
        "  # remove punctuation from lower_case and save to a variable\n",
        "  # don't worry too much if you don't understand the code in this line. \n",
        "  lower_case_no_punctuation = ''.join([character for character in lower_case if character not in punctuation])\n",
        "  # return the new text to the user\n",
        "  return lower_case_no_punctuation\n",
        "\n",
        "\n",
        "\n",
        "# define a function to calculate lexical diversity\n",
        "def lexical_diversity(tokens):\n",
        "  # return the result of dividing the length \n",
        "  return len(set(tokens))/len(tokens)\n",
        "\n",
        "\n",
        "\n",
        "def pipeline(string_input):\n",
        "  # first lowercase the string and clear punctuation using our preprocess function (defined above)\n",
        "  preprocess_string = preprocess(string_input)\n",
        "\n",
        "  # now use NLTK to tokenize the preprocessed text\n",
        "  tokenized_string = nltk.word_tokenize(preprocess_string)\n",
        "\n",
        "  # calculate the diversity function (defined above)\n",
        "  ld = lexical_diversity(tokenized_string)\n",
        "\n",
        "  # pos tag the tokens\n",
        "  pos_tagged_string = nltk.pos_tag(tokenized_string)\n",
        "\n",
        "  # calculate frequency of words and tags\n",
        "  fdist = nltk.FreqDist(pos_tagged_string)\n",
        "\n",
        "  # output some information about the text\n",
        "  print(f\"\"\"\n",
        "  Length:\\t{len(tokenized_string)}\\n\n",
        "  Lexical Diversity:\\t{ld}\\n\n",
        "  Top 5 Frequent Words:\\t{fdist.most_common(5)}\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "egx3mbaPNeMI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila, we can now operate on our text as we have been doing. "
      ],
      "metadata": {
        "id": "Ir3XzcdzZNEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(tmoom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "219jCxjcZCee",
        "outputId": "c61731fa-6c55-4b33-88ac-b6778f7873a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Length:\t808\n",
            "\n",
            "  Lexical Diversity:\t0.37995049504950495\n",
            "\n",
            "  Top 5 Frequent Words:\t[(('the', 'DT'), 42), (('meat', 'NN'), 40), (('?', '.'), 29), (('to', 'TO'), 21), (('they', 'PRP'), 18)]\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you choose to mount your Google Drive, you only need to change the file path to match wherever the file is on your Google Drive.\n",
        "\n",
        "The start of your file path will always be `content/drive/MyDrive/...`, where the `...` represents the root level of your Google Drive. \n",
        "\n",
        "So if you had `tmoom.txt` saved in a folder named `texts` in your Google Drive, the filepath would be:\n",
        "\n"
      ],
      "metadata": {
        "id": "JbrhzJxAZ8JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in case you want to mount your drive instead of manual upload. \n",
        "tmoom2 = open('/content/drive/MyDrive/texts/tmoom.txt').read()"
      ],
      "metadata": {
        "id": "HUVL5eS2c84y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}